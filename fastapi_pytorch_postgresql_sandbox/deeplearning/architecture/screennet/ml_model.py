""" ml_model """
from __future__ import annotations

import pathlib

# ---------------------------------------------------------------------------
import torch
import torchvision
from icecream import ic
from rich import print

from fastapi_pytorch_postgresql_sandbox.deeplearning.common import devices
from fastapi_pytorch_postgresql_sandbox.settings import Settings, settings

# ---------------------------------------------------------------------------
# Import rich and whatever else we need
# %load_ext rich
# %matplotlib inline


# better_exceptions.hook()

# console: Console = Console()
# ---------------------------------------------------------------------------


assert int(torch.__version__.split(".")[1]) >= 12, "torch version should be 1.12+"
assert (
    int(torchvision.__version__.split(".")[1]) >= 13
), "torchvision version should be 0.13+"
# print(f"torch version: {torch.__version__}")
# print(f"torchvision version: {torchvision.__version__}")
# ---------------------------------------------------------------------------

# Continue with regular imports
import mlxtend
import torch
import torchvision
from torch import nn

from fastapi_pytorch_postgresql_sandbox.deeplearning.architecture.screennet.config import (
    PATH_TO_BEST_MODEL,
)

# breakpoint()
# from going_modular import data_setup, engine, utils  # pylint: disable=no-name-in-module

# Try to get torchinfo, install it if it doesn't work


# print(f"mlxtend version: {mlxtend.__version__}")
assert (
    int(mlxtend.__version__.split(".")[1]) >= 19
), "mlxtend verison should be 0.19.0 or higher"

from pathlib import Path
from typing import List, Tuple

import numpy as np

# SOURCE: https://github.com/rasbt/deeplearning-models/blob/35aba5dc03c43bc29af5304ac248fc956e1361bf/pytorch_ipynb/helper_evaluate.py
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.optim

# # Import accuracy metric
# from helper_functions import (  # Note: could also use torchmetrics.Accuracy()
#     accuracy_fn,
#     plot_loss_curves,
# )
import torch.profiler
import torch.utils.data
import torch.utils.data.distributed
import torchvision.models as models

# SOURCE: https://github.com/pytorch/pytorch/issues/78924
torch.set_num_threads(1)


def validate_seed(seed: int) -> None:
    """Sets random sets for torch operations.

    Note: Recall a random seed is a way of flavouring the randomness generated by a computer. They aren't necessary to always set when running machine learning code, however, they help ensure there's an element of reproducibility (the numbers I get with my code are similar to the numbers you get with your code). Outside of an education or experimental setting, random seeds generally aren't required.

    Args:
        seed (int): _description_
    """
    ic(seed, type(seed))
    devices.seed_everything(seed)

    # Nevermind, the unexpected behaviour is from cpu not mps. I was using from_numpy which the document indicate it will create a tensor buffer that shared memory space (I didn't catch this at first). This shared memory space seems to only be valid for cpu version, and nor mps version

    # I tried switching cpu, and mps order around without using copy(), and got expected result, but the base_tensor value is modified after cpu computation. So in my conclusion, using copy() with from_numpy is needed to have consistent behaviour on both cpu, and mps

    # https://github.com/pytorch/pytorch/issues/77988
    # Test seed set correctly
    coeff = 0.5
    base_tensor = np.random.rand(100, 100).astype(np.float32)
    grad_tensor = np.random.rand(100, 100).astype(np.float32)

    device = "cpu"
    cpu_tensor = torch.from_numpy(base_tensor.copy()).to(device)  # Change this line
    cpu_tensor.requires_grad = True
    cpu_tensor.grad = torch.from_numpy(grad_tensor.copy()).to(
        device,
    )  # Change this line

    with torch.no_grad():
        cpu_tensor.add_(-coeff * cpu_tensor.grad)

    if torch.backends.mps.is_available():
        device = "mps"
        mps_tensor = torch.from_numpy(base_tensor.copy()).to(device)  # Change this line
        mps_tensor.requires_grad = True
        mps_tensor.grad = torch.from_numpy(grad_tensor.copy()).to(
            device,
        )  # Change this line

        with torch.no_grad():
            mps_tensor.add_(-coeff * mps_tensor.grad)

        print(cpu_tensor.detach().cpu().numpy() - mps_tensor.detach().cpu().numpy())


# ------------------------------------------------------------


# boss: use this to instantiate a new model class with all the proper setup as before
def create_effnetb0_model(
    device: str,
    class_names: List[str],
    settings: Settings,
) -> torch.nn.Module:
    """Create an instance of pretrained model EfficientNet_B0, freeze all base layers and define classifier. Return model class

    Args:
        device (str): _description_
        class_names (List[str]): _description_

    Returns:
        _type_: _description_
    """
    # NEW: Setup the model with pretrained weights and send it to the target device (torchvision v0.13+)
    weights = models.__dict__[settings.model_weights].DEFAULT
    # weights = (
    #     torchvision.models.EfficientNet_B0_Weights.DEFAULT
    # )  # .DEFAULT = best available weights
    model = torchvision.models.efficientnet_b0(weights=weights).to(device)

    # Freeze all base layers in the "features" section of the model (the feature extractor) by setting requires_grad=False
    for param in model.features.parameters():
        param.requires_grad = False

    # Set the manual seeds
    validate_seed(settings.seed)

    # Get the length of class_names (one output unit for each class)
    output_shape = len(class_names)

    # Recreate the classifier layer and seed it to the target device
    model.classifier = torch.nn.Sequential(
        torch.nn.Dropout(p=0.2, inplace=True),
        torch.nn.Linear(
            in_features=1280,
            out_features=output_shape,  # same number of output units as our number of classes
            bias=True,
        ),
    ).to(device)

    # 5. Give the model a name
    model.name = "effnetb0"
    print(f"[INFO] Created new {model.name} model.")

    return model


# wrapper function of common code
def run_save_model_for_inference(model: torch.nn.Module) -> Tuple[pathlib.PosixPath]:
    """Save model to disk

    Args:
        model (torch.nn.Module): _description_

    Returns:
        Tuple[pathlib.PosixPath]: _description_
    """
    ic("Saving model to disk ...")
    path_to_model = save_model_to_disk("ScreenNetV1", model)
    ic(path_to_model)
    return path_to_model


# wrapper function of common code
def run_get_model_for_inference(
    model: torch.nn.Module,
    device: torch.device,
    class_names: List[str],
    path_to_model: pathlib.PosixPath,
    settings: Settings,
) -> torch.nn.Module:
    """wrapper function to load model .pth file from disk

    Args:
        model (torch.nn.Module): _description_
        device (torch.device): _description_
        class_names (List[str]): _description_

    Returns:
        Tuple[pathlib.PosixPath, torch.nn.Module]: _description_
    """
    return load_model_for_inference(
        path_to_model,
        device,
        class_names,
        settings,
    )


# SOURCE: https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference
# Saving & Loading Model for Inference
def save_model_to_disk(my_model_name: str, model: torch.nn.Module) -> Path:
    """_summary_

    Args:
        my_model_name (str): _description_
        model (torch.nn.Module): _description_

    Returns:
        Path: _description_
    """
    # Create models directory (if it doesn't already exist), see: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir
    MODEL_PATH = Path("models")
    MODEL_PATH.mkdir(
        parents=True,  # create parent directories if needed
        exist_ok=True,  # if models directory already exists, don't error
    )

    # Create model save path
    MODEL_NAME = f"{my_model_name}.pth"
    MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME

    # Save the model state dict
    print(f"Saving model to: {MODEL_SAVE_PATH}")
    torch.save(
        obj=model.state_dict(),  # only saving the state_dict() only saves the learned parameters
        f=MODEL_SAVE_PATH,
    )
    print(f"Model saved to path {MODEL_SAVE_PATH} successfully.")
    return MODEL_SAVE_PATH


# NOTE: https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference
def load_model_for_inference(
    save_path: str,
    device: str,
    class_names: List[str],
    settings: Settings,
) -> nn.Module:
    """_summary_

    Args:
        save_path (str): _description_
        device (str): _description_
        class_names (List[str]): _description_
        settings (argparse.Namespace): _description_

    Returns:
        nn.Module: _description_
    """
    model = create_effnetb0_model(device, class_names, settings)
    model.load_state_dict(torch.load(save_path))
    model.eval()
    print(f"Model loaded from path {save_path} successfully.")
    # Get the model size in bytes then convert to megabytes
    model_size = Path(save_path).stat().st_size // (1024 * 1024)
    print(f"EfficientNetB2 feature extractor model size: {model_size} MB")

    # get_model_summary(model)
    return model


# SOURCE: https://github.com/a-sasikumar/image_caption_errors/blob/d583dc77cfa9938bb15297b3096a959fe6084b66/models/model.py
def load_model_from_disk(save_path: str, empty_model: nn.Module) -> nn.Module:
    """_summary_

    Args:
        save_path (str): _description_
        empty_model (nn.Module): _description_

    Returns:
        nn.Module: _description_
    """
    # Loading Model for Inference
    empty_model.load_state_dict(torch.load(save_path))
    # Remember that you must call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results.
    empty_model.eval()
    print(f"Model loaded from path {save_path} successfully.")
    return empty_model


class ImageClassifier:
    """_summary_"""

    def __init__(
        self,
        seed=None,
        path_to_model: str = PATH_TO_BEST_MODEL,
        settings=settings,
    ):
        """_summary_

        Args:
            seed (_type_, optional): _description_. Defaults to None.
        """
        self.class_names: List[str] = settings.class_names
        self.path_to_model: str = path_to_model
        self.settings: Settings = settings
        self.device = devices.get_optimal_device(settings)
        self.weights = models.__dict__[settings.model_weights].DEFAULT
        self.auto_transforms = self.weights.transforms()
        self.model = models.__dict__[settings.arch](weights=settings.weights).to(
            settings.device,
        )
        self.model.name = settings.arch

    # def predict(self):

    def load_base_model(self) -> None:
        """_summary_"""

    def set_seed(self) -> None:
        """_summary_"""
        validate_seed(settings.seed)

    # def load_weigths(self) -> None:
    #     """_summary_"""

    #     # path_to_model = save_model_to_disk("ScreenNetV1", model)

    #     loaded_model_for_inference: nn.Module
    #     loaded_model_for_inference = run_get_model_for_inference(
    #         model, device, self.class_names, path_to_model, settings
    #     )

    def load_model(self, pretrained=True) -> None:
        """_summary_

        Args:
            pretrained (bool, optional): _description_. Defaults to True.
        """
        if pretrained:
            ic(f"=> using pre-trained model '{settings.arch}'")

            device = devices.get_optimal_device(settings)

            weights = models.__dict__[settings.model_weights].DEFAULT
            weights.transforms()
            model = models.__dict__[settings.arch](weights=weights).to(device)
        else:
            ic(f"=> creating model '{settings.arch}'")
            # breakpoint()
            # weights = models.__dict__[settings.model_weights].DEFAULT.to(device)
            # auto_transforms = weights.transforms()
            model = models.__dict__[settings.arch]()
        model.name = settings.arch
        # Freeze all base layers in the "features" section of the model (the feature extractor) by setting requires_grad=False
        for param in model.features.parameters():
            param.requires_grad = False

        # Get the length of class_names (one output unit for each class)
        output_shape = len(settings.class_names)

        # Recreate the classifier layer and seed it to the target device
        model.classifier = torch.nn.Sequential(
            torch.nn.Dropout(p=0.2, inplace=True),
            torch.nn.Linear(
                in_features=1280,
                out_features=output_shape,  # same number of output units as our number of classes
                bias=True,
            ),
        ).to(device)

        ic(next(model.parameters()).device)

        nn.CrossEntropyLoss()

        optimizer = torch.optim.Adam(model.parameters(), lr=settings.lr)

        # this is the part we really need
        if settings.weights:
            ic(f"loading weights from -> {settings.weights}")
            # loaded_model: nn.Module
            model = run_get_model_for_inference(
                model,
                device,
                self.class_names,
                settings.weights,
                settings,
            )

    def configure_optimal_settings(self) -> None:
        """_summary_"""
        if not torch.cuda.is_available() and not torch.backends.mps.is_available():
            ic("using CPU, this will be slow")
        elif settings.gpu is not None and torch.cuda.is_available():
            ic("GPU enabled")
            torch.cuda.set_device(settings.gpu)
            model = model.cuda(settings.gpu)
        elif torch.backends.mps.is_available():
            ic("MPS mode enabled")
            device = torch.device("mps")
            model = model.to(device)
        elif settings.arch.startswith("alexnet") or settings.arch.startswith("vgg"):
            ic(f"Using alexnet or vgg -> {settings.arch}")
            model.features = torch.nn.DataParallel(model.features)
            model.cuda()
        else:
            model = torch.nn.DataParallel(model).cuda()

        if torch.cuda.is_available():
            if settings.gpu:
                device = torch.device(f"cuda:{settings.gpu}")
            else:
                device = torch.device("cuda")
        elif torch.backends.mps.is_available():
            device = torch.device("mps")
        else:
            device = torch.device("cpu")
